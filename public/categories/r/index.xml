<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>R on Maximuming At The Peak</title>
    <link>/categories/r/</link>
    <description>Recent content in R on Maximuming At The Peak</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 27 Dec 2018 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/categories/r/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Visualizing Prophet&#39;s changepoint.prior.scale</title>
      <link>/2018/12/27/visualizing-prophet-s-changepoint-prior-scale/</link>
      <pubDate>Thu, 27 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/27/visualizing-prophet-s-changepoint-prior-scale/</guid>
      <description>As a followup to my post about visualizing Prophet Forecast Output, this post will do the same process of combing purrr/furrr and prophet to run a lot of forecasts in a more structured way and then visualizing the results with gganimate.
The purpose of this post will be to provide a range of input values to the changepoint.prior.scale parameter and observe what it does to Prophet’s forecast and it’s confidence intervals.</description>
    </item>
    
    <item>
      <title>Visualizing Prophet Forecasts (And Errors) with purrr, and gganimate</title>
      <link>/2018/12/22/visualizing-prophet-with-gganimate/</link>
      <pubDate>Sat, 22 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/12/22/visualizing-prophet-with-gganimate/</guid>
      <description>Credit Where Credit is Due The idea and methods of using purrr with Prophet here were not my own. I copied this methodology from a coworker who was kind enough to share this approach below which leverages map() functions to more cleanly structure our input data and apply the forecast function over it many times.
 Part 1: Background What Am I Doing? Facebook has developed an open-source forecasting library for Python and R called Prophet authored by Sean J.</description>
    </item>
    
    <item>
      <title>Expand Grids with...`expand.grid()`</title>
      <link>/2018/02/28/expand-grids-with-expand-grid/</link>
      <pubDate>Wed, 28 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/28/expand-grids-with-expand-grid/</guid>
      <description>COPY / PASTE HERE:   expand.grid(seq(0,10), seq(0,10), seq(0,10))
 Another quick one but useful. I recently had a use-case at work where I was plotting datapoints of events on a 2d canvas. We had thousands of clients and a unique canvas where I needed to paint thousands of events for each client. I decided to normalize everything to a 0-100 scale across the x and y dimensions. The problem which prompted me to use expand.</description>
    </item>
    
    <item>
      <title>Grouped Summary With `purrr::map()`</title>
      <link>/2018/02/23/grouped-summary-with-purrr-map/</link>
      <pubDate>Fri, 23 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/2018/02/23/grouped-summary-with-purrr-map/</guid>
      <description>COPY / PASTE HERE:   mtcars %&amp;gt;% split(.$cyl) %&amp;gt;% map(summary)
 This is a quick one and sourced from this SO answer but I kept googling it so wanted to just put it up here.
require(dplyr) require(purrr) mtcars %&amp;gt;% split(.$cyl) %&amp;gt;% map(summary) ## $`4` ## mpg cyl disp hp ## Min. :21.40 Min. :4 Min. : 71.10 Min. : 52.00 ## 1st Qu.:22.80 1st Qu.:4 1st Qu.: 78.</description>
    </item>
    
    <item>
      <title>Logarithmic Bucketing in R</title>
      <link>/2017/12/07/logarithmic-bucketing/</link>
      <pubDate>Thu, 07 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/12/07/logarithmic-bucketing/</guid>
      <description>COPY / PASTE HERE:   c(0,10^(1:7))
  paste0(‘&amp;lt;=’,scales::comma(c(10^(1:7))))
 Some of the datasets I deal with are lognormally distributed. Basically what this means is they follow the 80/20 rule where 80% of the sum of the values comes from the top 20% of the observations when sorted descending. There are a lot of observations with very low values relative to the range.</description>
    </item>
    
    <item>
      <title>Plot Grouped Indexed Time-Series Changes</title>
      <link>/2017/12/05/indexed-time-series-changes/</link>
      <pubDate>Tue, 05 Dec 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/12/05/indexed-time-series-changes/</guid>
      <description>COPY / PASTE HERE:   mutate( index = value / value[1] - 1 )
 One thing I do at my job all. the. time is index time-series together in order to compare relative changes in a given time window. The problem statement is usually when I’m comparing the price changes of two assets with vastly different prices. So trying to plot something moving from $1MM to $1.</description>
    </item>
    
    <item>
      <title>The Train Schedule</title>
      <link>/2017/10/01/the-train-schedule/</link>
      <pubDate>Sun, 01 Oct 2017 00:00:00 +0000</pubDate>
      
      <guid>/2017/10/01/the-train-schedule/</guid>
      <description>This website needs content. I don’t really know what to write at the moment so I’m going to just start with a simple recreation of a famous old ( originally made 137 years ago) visualization to get things rolling. I could have chosen to write about Napoleon’s infamous march but plenty of people wrote about that already.
I recently purchased a copy of https://www.amazon.co.uk/Visual-Display-Quantitative-Information/dp/0961392142 by Edward Tufte. The cover image is a photo of E.</description>
    </item>
    
  </channel>
</rss>